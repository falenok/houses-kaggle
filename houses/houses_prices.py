# -*- coding: utf-8 -*-
"""Houses prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OE2QPMcnYZ5XqR8sb150suyWxRXPChsQ
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.model_selection import train_test_split,cross_validate, GridSearchCV

from sklearn.linear_model import LinearRegression,Lasso,Ridge,BayesianRidge
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor



import warnings
warnings.filterwarnings("ignore")


from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn import linear_model
from sklearn.neighbors import (NeighborhoodComponentsAnalysis, KNeighborsClassifier)
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import xgboost as xgb
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.preprocessing import StandardScaler,OrdinalEncoder
from sklearn.metrics import mean_squared_error

df_train = pd.read_csv('/content/train.csv', index_col='Id')
df_test = pd.read_csv('/content/test.csv', index_col='Id')
sample_submission = pd.read_csv('/content/sample_submission.csv')
df = pd.concat([df_train, df_test], axis=0)

df

plt.figure(figsize=(15,10))
plt.xlabel("Цена")
plt.ylabel("Количество значений")
plt.hist(df["SalePrice"])

plt.figure(figsize=(15,10))
plt.xlabel("Десятичный логарифм цены")
plt.ylabel("Количество значений")
plt.hist(np.log10(df["SalePrice"]))

"""В датасете 43 категориальные колонки. При этом некоторые из них можно проранжировать, поскольку значения в них являются сравнительными (плохое => хорошее)"""

range_cat = ['ExterQual', 'ExterCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond' ]

null_values = df.isna().sum()

"""Рассмотрим количество нулевых значений в различных типах данных. Сначала числовые колонки."""

df.select_dtypes(include=['int64','float64']).isna().sum()/df.shape[0]*100

"""Для простоты заменим пустые значения для категорий с малым количеством пропуском средними значнеиями."""

low_null = ['GarageArea', 'GarageCars', 'BsmtHalfBath', 'BsmtFullBath', 'TotalBsmtSF', 'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1', 'MasVnrArea']

for i in low_null:
  df[i].fillna(df[i].mean(), inplace=True)

df.select_dtypes(include=['int64','float64']).isna().sum()/df.shape[0]*100

"""В колонке GarageYrBlt достаточно много пустых значений, но поскольку эта категория является датой постройки гаража, можно предположить, что в большинстве случаев она будет совпадать с датой постройки дома. Поэтому заполним пустые значения исходя из года постройки дома."""

df['GarageYrBlt'].fillna(df['YearBuilt'], inplace=True)

df.select_dtypes(include=['int64','float64']).isna().sum()/df.shape[0]*100

"""Осталась одна колонка LotFrontage, содержащая 16% пустых значений. Можно заменить средним, но я предпочту другой вариант. Поскольку она означает площадь примыкающей к дому территории, мы можем предположить, что это есть функция от площади дома. Поэтому используем линейную регрессию для определения коэффицентов и домножим на площадь дома."""

lm = LinearRegression()
lm_X = df[df['LotFrontage'].notnull()]['LotArea'].values.reshape(-1,1)
lm_y = df[df['LotFrontage'].notnull()]['LotFrontage'].values
lm.fit(lm_X,lm_y)
df['LotFrontage'].fillna((df['LotArea'] * lm.coef_[0] + lm.intercept_), inplace=True)

df.select_dtypes(include=['int64','float64']).isna().sum()/df.shape[0]*100

"""Теперь посмотрим на пустые значения в категориальных колонках."""

df.select_dtypes(include=['object']).isna().sum()/df.shape[0]*100

"""Заполнять колонки с более чем 50% пустых значений смысла не имеет. Особенно если эти колонки не являются принципильно важными, поэтому их лучше отбросить."""

df.drop(['MiscFeature','Fence','PoolQC','FireplaceQu','Alley'], axis=1, inplace=True)

df.select_dtypes(include=['object']).isna().sum()/df.shape[0]*100

df[range_cat].isna().sum()/df.shape[0]*100

"""Заменим nan на отсутствющее значение ('none')"""

for i in range_cat:
  df[i].fillna('None', inplace=True)

df.select_dtypes(include=['object']).isna().sum()/df.shape[0]*100

df['GarageType'].fillna('None', inplace=True)
df['GarageFinish'].fillna('None', inplace=True)

"""Оставшиеся пустые значениями заполняем модой."""

obj_col = df.select_dtypes(include=['object']).columns

obj_col[0]

for i in obj_col:
  df[i].fillna(df[i].mode()[0], inplace=True)

df['MSZoning'].mode().values

df['MSZoning'].fillna(df['MSZoning'].mode(), inplace=True)

df.select_dtypes(include=['object']).isna().sum()/df.shape[0]*100

df['MSZoning'].unique()

"""С пустыми значениями разобрались, теперь надо перейти к кодированию категориальных данных. Начнем с ранжируемых значений."""

cate1 = ["BsmtCond"]
cate1_item = ['None',"Po", "Fa", "TA", "Gd"]

cate2 = ["BsmtExposure"]
cate2_item = ['None','No','Mn','Av','Gd']

cate3 = ["BsmtQual"]
cate3_item = ['None',"Fa","TA","Gd", "Ex"]

cate4 = ["ExterCond", "HeatingQC"]
cate4_item = ["Po", "Fa", "TA", "Gd", "Ex"]

cate5 = ["ExterQual", "KitchenQual"]
cate5_item = ["None","Fa", "TA", "Gd", "Ex"]

cate6 = ["GarageQual", "GarageCond"]
cate6_item = ['None',"Po", "Fa", "TA", "Gd", "Ex"]

cate7 = ["BsmtFinType1", "BsmtFinType2"]
cate7_item = ['None',"Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ"]

cate = [cate1,cate2,cate3,cate4,cate5,cate6,cate7]
cate_item = [cate1_item,cate2_item,cate3_item,cate4_item,cate5_item,cate6_item,cate7_item]

df['KitchenQual'].unique()

"""Используем ordinalencoder"""

for idx in range(len(cate)):
    encoder = OrdinalEncoder(categories = [cate_item[idx]])
    
    for col in cate[idx]:
        df[col] = encoder.fit_transform(df[[col]])

df.info()

"""Перейдем к кодированию оставшихся категориальных переменных."""

cate_feat = list(df.select_dtypes(include = [object]))

cate_feat

for i in cate_feat:
  print(len(df[i].unique()))

"""Для колонок с уникальным значением меньше 6 применим горячее кодирование."""

cate_one_hot = list()
cate_target_var = list()

for col in df[cate_feat].columns:
    if len(df[col].unique()) <6:
        cate_one_hot.append(col)
    else:
        cate_target_var.append(col)

cate_one_hot

dummies_one_hot = pd.get_dummies(df[cate_one_hot], drop_first = True)
dummies_one_hot

"""Для остальных"""

def one_hot(df):
    
    for col in df:
        top_10 = [item for item in df[col].value_counts().sort_values(ascending = False).head(6).index]
        
        for label in top_10:
            df[label] = np.where(df[col]==label,1,0)
            
    return df

df_tar_var = one_hot(df[cate_target_var])
df_tar_var.drop(cate_target_var,axis = 1,inplace = True)

"""объединяем"""

df_final = pd.concat([df_tar_var,dummies_one_hot,df],axis = 1)
df_final.drop(cate_feat,axis = 1, inplace = True)

df_final

df_final_train = df_final[df_final['SalePrice'].notna()]
df_final_test = df_final[df_final['SalePrice'].isna()]

df_final_train['SalePrice'] = df_final_train['SalePrice'].apply(lambda x: np.log10(x))

"""Сплитим тренировочный датасет на трейн и тест выборки"""

X = df_final_train.drop(['SalePrice'], axis=1)
y = df_final_train['SalePrice']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

"""Проводим кросс-валидацию для выбора классификатора"""

pipe = Pipeline([('classifier', LogisticRegression())])


param_grid = [
    {'classifier' : [LogisticRegression()]
    },
    {'classifier' : [RandomForestRegressor()]
    },
    {'classifier' : [linear_model.LassoLars()]
    },
    {'classifier' : [GaussianNB()]
    },
    {'classifier' : [GradientBoostingRegressor()]
    }
    
]



clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1, scoring='neg_root_mean_squared_error')



best_clf = clf.fit(X_train, y_train)

best_clf.best_estimator_

best_clf.best_score_

"""Как результат, видно что лучшим регрессором выступает GradientBoostingRegressor."""

gbr = GradientBoostingRegressor()
gbr.fit(X_train, y_train)
y_pred = gbr.predict(X_test)

mean_squared_error(y_test, y_pred, squared=False)

parameters = {'learning_rate': [0.01,0.02,0.03,0.04],
                  'subsample'    : [0.9, 0.5, 0.2, 0.1],
                  'n_estimators' : [100,500,1000, 1500],
                  'max_depth'    : [4,6,8,10]
                 }
gbr = GradientBoostingRegressor()
grid_GBR = GridSearchCV(estimator=gbr, param_grid = parameters, cv = 2, n_jobs=-1)
grid_GBR.fit(X_train, y_train)

gbr = GradientBoostingRegressor()
grid_GBR = GridSearchCV(estimator=gbr, param_grid = parameters, cv = 2, n_jobs=-1)
grid_GBR.fit(X_train, y_train)

print(" Results from Grid Search ")
print("\n The best estimator across ALL searched params:\n",grid_GBR.best_estimator_)
print("\n The best score across ALL searched params:\n",grid_GBR.best_score_)
print("\n The best parameters across ALL searched params:\n",grid_GBR.best_params_)



gbr = GradientBoostingRegressor(learning_rate=0.04, max_depth=4, n_estimators=1500, subsample=0.9)
gbr.fit(X_train, y_train)
y_pred = gbr.predict(X_test)

mean_squared_error(y_test, y_pred, squared=False)

gbr = GradientBoostingRegressor(learning_rate=0.04, max_depth=4, n_estimators=1500, subsample=0.9)
gbr.fit(X, y)
y_pred = gbr.predict(df_final_test.drop(['SalePrice'], axis=1))

y_pred

sub = []
for i in y_pred:
  sub.append(10**i)

sample_submission['SalePrice']=sub
sample_submission.to_csv('newsub2.csv', index=False)

sample_submission

"""#Конец базовой модели"""

df_train = pd.read_csv('/content/train.csv', index_col='Id')
df_test = pd.read_csv('/content/test.csv', index_col='Id')
sample_submission = pd.read_csv('/content/sample_submission.csv')
df = pd.concat([df_train, df_test], axis=0)

df

df['SalePrice'] = df['SalePrice'].apply(lambda x: np.log10(x))

"""Избавляемся от нулей."""

range_cat = ['ExterQual', 'ExterCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond' ]
low_null = ['GarageArea', 'GarageCars', 'BsmtHalfBath', 'BsmtFullBath', 'TotalBsmtSF', 'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1', 'MasVnrArea']
for i in low_null:
  df[i].fillna(df[i].mean(), inplace=True)
df['GarageYrBlt'].fillna(df['YearBuilt'], inplace=True)

lm = LinearRegression()
lm_X = df[df['LotFrontage'].notnull()]['LotArea'].values.reshape(-1,1)
lm_y = df[df['LotFrontage'].notnull()]['LotFrontage'].values
lm.fit(lm_X,lm_y)
df['LotFrontage'].fillna((df['LotArea'] * lm.coef_[0] + lm.intercept_), inplace=True)

df.drop(['MiscFeature','Fence','PoolQC','FireplaceQu','Alley'], axis=1, inplace=True)

for i in range_cat:
  df[i].fillna('None', inplace=True)

df['GarageType'].fillna('None', inplace=True)
df['GarageFinish'].fillna('None', inplace=True)

obj_col = df.select_dtypes(include=['object']).columns

for i in obj_col:
  df[i].fillna(df[i].mode()[0], inplace=True)

cate1 = ["BsmtCond"]
cate1_item = ['None',"Po", "Fa", "TA", "Gd"]

cate2 = ["BsmtExposure"]
cate2_item = ['None','No','Mn','Av','Gd']

cate3 = ["BsmtQual"]
cate3_item = ['None',"Fa","TA","Gd", "Ex"]

cate4 = ["ExterCond", "HeatingQC"]
cate4_item = ["Po", "Fa", "TA", "Gd", "Ex"]

cate5 = ["ExterQual", "KitchenQual"]
cate5_item = ["None","Fa", "TA", "Gd", "Ex"]

cate6 = ["GarageQual", "GarageCond"]
cate6_item = ['None',"Po", "Fa", "TA", "Gd", "Ex"]

cate7 = ["BsmtFinType1", "BsmtFinType2"]
cate7_item = ['None',"Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ"]

cate = [cate1,cate2,cate3,cate4,cate5,cate6,cate7]
cate_item = [cate1_item,cate2_item,cate3_item,cate4_item,cate5_item,cate6_item,cate7_item]

for idx in range(len(cate)):
    encoder = OrdinalEncoder(categories = [cate_item[idx]])
    
    for col in cate[idx]:
        df[col] = encoder.fit_transform(df[[col]])

df

"""Сплитим по типам объектов"""

df['MSZoning'].unique()

df_RL = df[df['MSZoning']=='RL']
df_RM = df[df['MSZoning']=='RM']
df_C = df[df['MSZoning']=='C (all)']
df_FV = df[df['MSZoning']=='FV']
df_RH = df[df['MSZoning']=='RH']

df_RL.drop(['MSZoning'], axis=1, inplace=True)
df_RM.drop(['MSZoning'], axis=1, inplace=True)
df_C.drop(['MSZoning'], axis=1, inplace=True)
df_FV.drop(['MSZoning'], axis=1, inplace=True)
df_RH.drop(['MSZoning'], axis=1, inplace=True)

df_RL

"""Работает с категориальными данными."""

def encoding(x):
  dummies_one_hot = pd.get_dummies(x[cate_one_hot], drop_first = True)


  df_tar_var = one_hot(x[cate_target_var])
  df_tar_var.drop(cate_target_var,axis = 1,inplace = True)

  df_tar_var = one_hot(x[cate_target_var])
  df_tar_var.drop(cate_target_var,axis = 1,inplace = True)

  df_final = pd.concat([df_tar_var,dummies_one_hot,x],axis = 1)
  df_final.drop(cate_feat,axis = 1, inplace = True)

  return df_final

def one_hot(df):
    
    for col in df:
        top_10 = [item for item in df[col].value_counts().sort_values(ascending = False).head(6).index]
        
        for label in top_10:
            df[label] = np.where(df[col]==label,1,0)
            
    return df

cate_one_hot = list()
cate_target_var = list()
cate_feat = list(df_RL.select_dtypes(include = [object]))
for col in df_RL[cate_feat].columns:
    if len(df_RL[col].unique()) <6:
        cate_one_hot.append(col)
    else:
        cate_target_var.append(col)

df_RL = encoding(df_RL)
df_RM = encoding(df_RM)
df_C = encoding(df_C)
df_FV = encoding(df_FV)
df_RH = encoding(df_RH)

df_RH.info()

df_RL

df_RL_train = df_RL[df_RL['SalePrice'].notna()]
df_RL_test = df_RL[df_RL['SalePrice'].isna()]

df_RM_train = df_RM[df_RM['SalePrice'].notna()]
df_RM_test = df_RM[df_RM['SalePrice'].isna()]

df_C_train = df_C[df_C['SalePrice'].notna()]
df_C_test = df_C[df_C['SalePrice'].isna()]

df_FV_train = df_FV[df_FV['SalePrice'].notna()]
df_FV_test = df_FV[df_FV['SalePrice'].isna()]

df_RH_train = df_RH[df_RH['SalePrice'].notna()]
df_RH_test = df_RH[df_RH['SalePrice'].isna()]

def modeling_func(train):
  X = train.drop(['SalePrice'], axis=1)
  y = train['SalePrice']

  #data_test = test.drop(['SalePrice'], axis=1)

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)
  pipe = Pipeline([('classifier', LinearRegression())])


  param_grid = [
      {'classifier' : [LinearRegression()]
      },
      {'classifier' : [RandomForestRegressor()]
      },
      {'classifier' : [linear_model.LassoLars()]
      },
      {'classifier' : [GaussianNB()]
      },
      {'classifier' : [GradientBoostingRegressor()]
      }
      
  ]


  clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1, scoring='neg_root_mean_squared_error')



  best_clf = clf.fit(X_train, y_train)

  print(best_clf.best_estimator_)
  print(best_clf.best_score_)

trains = [df_RL_train, df_RM_train, df_C_train, df_FV_train, df_RH_train]
tests = [df_RL_test, df_RM_test, df_C_test, df_FV_test, df_RH_test]

df_RL_test

for i in trains:
  modeling_func(i)

def modeling_func(train):
  X = train.drop(['SalePrice'], axis=1)
  y = train['SalePrice']

  #data_test = test.drop(['SalePrice'], axis=1)

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)

  parameters = {'learning_rate': [0.01,0.02,0.03,0.04],
                    'subsample'    : [0.9, 0.5, 0.2, 0.1],
                    'n_estimators' : [100,500,1000, 1500],
                    'max_depth'    : [4,6,8,10]
                  }
  gbr = GradientBoostingRegressor()
  grid_GBR = GridSearchCV(estimator=gbr, param_grid = parameters, cv = 2, n_jobs=-1)
  grid_GBR.fit(X_train, y_train)
  print(" Results from Grid Search ")
  print("\n The best estimator across ALL searched params:\n",grid_GBR.best_estimator_)
  print("\n The best score across ALL searched params:\n",grid_GBR.best_score_)
  print("\n The best parameters across ALL searched params:\n",grid_GBR.best_params_)

{'learning_rate': 0.02, 'max_depth': 10, 'n_estimators': 500, 'subsample': 0.1}
{'learning_rate': 0.03, 'max_depth': 4, 'n_estimators': 1500, 'subsample': 0.2}
{'learning_rate': 0.04, 'max_depth': 10, 'n_estimators': 1500, 'subsample': 0.9}
{'learning_rate': 0.04, 'max_depth': 8, 'n_estimators': 500, 'subsample': 0.9}
{'learning_rate': 0.02, 'max_depth': 4, 'n_estimators': 1000, 'subsample': 0.5}

for i in trains:
  modeling_func(i)

def final_modeling(train, test):

  gbr = GradientBoostingRegressor(learning_rate=0.04, max_depth=4, n_estimators=1500, subsample=0.9)
  gbr.fit(train.drop(['SalePrice'], axis=1), train['SalePrice'])
  y_pred = gbr.predict(test.drop(['SalePrice'], axis=1))

  sub = []
  for i in y_pred:
    sub.append(10**i)
  test['SalePrice'] = sub
  return test

# {'learning_rate': 0.02, 'max_depth': 10, 'n_estimators': 500, 'subsample': 0.1}
# {'learning_rate': 0.03, 'max_depth': 4, 'n_estimators': 1500, 'subsample': 0.2}
# {'learning_rate': 0.04, 'max_depth': 10, 'n_estimators': 1500, 'subsample': 0.9}
# {'learning_rate': 0.04, 'max_depth': 8, 'n_estimators': 500, 'subsample': 0.9}
# {'learning_rate': 0.02, 'max_depth': 4, 'n_estimators': 1000, 'subsample': 0.5}

gbr1 = GradientBoostingRegressor(learning_rate=0.02, max_depth=10, n_estimators=500, subsample=0.1)
gbr1.fit(df_RL_train.drop(['SalePrice'], axis=1), df_RL_train['SalePrice'])
y_pred1 = gbr1.predict(df_RL_test.drop(['SalePrice'], axis=1))
df_RL_test['SalePrice'] = y_pred1

gbr2 = GradientBoostingRegressor(learning_rate=0.03, max_depth=4, n_estimators=1500, subsample=0.2)
gbr2.fit(df_RM_train.drop(['SalePrice'], axis=1), df_RM_train['SalePrice'])
y_pred2 = gbr2.predict(df_RM_test.drop(['SalePrice'], axis=1))
df_RM_test['SalePrice'] = y_pred2

gbr3 = GradientBoostingRegressor(learning_rate=0.04, max_depth=10, n_estimators=1500, subsample=0.9)
gbr3.fit(df_C_train.drop(['SalePrice'], axis=1), df_C_train['SalePrice'])
y_pred3 = gbr3.predict(df_C_test.drop(['SalePrice'], axis=1))
df_C_test['SalePrice'] = y_pred3

gbr4 = GradientBoostingRegressor(learning_rate=0.04, max_depth=8, n_estimators=500, subsample=0.9)
gbr4.fit(df_FV_train.drop(['SalePrice'], axis=1), df_FV_train['SalePrice'])
y_pred4 = gbr4.predict(df_FV_test.drop(['SalePrice'], axis=1))
df_FV_test['SalePrice'] = y_pred4

gbr5 = GradientBoostingRegressor(learning_rate=0.02, max_depth=4, n_estimators=1000, subsample=0.5)
gbr5.fit(df_RH_train.drop(['SalePrice'], axis=1), df_RH_train['SalePrice'])
y_pred5 = gbr5.predict(df_RH_test.drop(['SalePrice'], axis=1))
df_RH_test['SalePrice'] = y_pred5

sub_RL = final_modeling(df_RL_train, df_RL_test)
sub_RM = final_modeling(df_RM_train, df_RM_test)
sub_C = final_modeling(df_C_train, df_C_test)
sub_FV = final_modeling(df_FV_train, df_FV_test)
sub_RH = final_modeling(df_RH_train, df_RH_test)

sub_RL = sub_RL['SalePrice']
sub_RM = sub_RM['SalePrice']
sub_C = sub_C['SalePrice']
sub_FV = sub_FV['SalePrice']
sub_RH = sub_RH['SalePrice']

final_sub = pd.concat([df_RL_test,df_RM_test, df_C_test, df_FV_test, df_RH_test], axis=0)

final_sub = final_sub['SalePrice']

final_sub['SalePrice'] = final_sub['SalePrice'].apply(lambda x: 10**x)

final_sub.to_csv('newsub4.csv', index=True)